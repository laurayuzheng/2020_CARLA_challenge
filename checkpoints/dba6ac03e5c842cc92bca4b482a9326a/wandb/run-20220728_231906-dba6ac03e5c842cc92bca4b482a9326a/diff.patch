Submodule carla_project contains modified content
diff --git a/carla_project/src/dataset.py b/carla_project/src/dataset.py
index d292820..4b871cf 100644
--- a/carla_project/src/dataset.py
+++ b/carla_project/src/dataset.py
@@ -44,7 +44,7 @@ def get_weights(data, key='speed', bins=4):
     return class_weights[classes]
 
 
-def get_dataset(dataset_dir, is_train=True, batch_size=128, num_workers=4, sample_by='none', **kwargs):
+def get_dataset(dataset_dir, is_train=True, batch_size=128, num_workers=4, sample_by='none', use_cpu=False, **kwargs):
     data = list()
     transform = transforms.Compose([
         get_augmenter() if is_train else lambda x: x,
@@ -67,7 +67,7 @@ def get_dataset(dataset_dir, is_train=True, batch_size=128, num_workers=4, sampl
     sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))
     data = torch.utils.data.ConcatDataset(data)
 
-    return Wrap(data, sampler, batch_size, 1000 if is_train else 100, num_workers)
+    return Wrap(data, sampler, batch_size, 1000 if is_train else 100, num_workers, use_cpu)
 
 
 def get_augmenter():
@@ -195,7 +195,7 @@ class CarlaDataset(Dataset):
 
             points.append(target)
 
-        points = torch.FloatTensor(points)
+        points = torch.FloatTensor(np.array(points))
         points = torch.clamp(points, 0, 256)
         points = (points / 256) * 2 - 1
 
diff --git a/carla_project/src/dataset_wrapper.py b/carla_project/src/dataset_wrapper.py
index de19b02..51ea2dd 100644
--- a/carla_project/src/dataset_wrapper.py
+++ b/carla_project/src/dataset_wrapper.py
@@ -9,19 +9,25 @@ def _repeater(dataloader):
             yield data
 
 
-def _dataloader(data, sampler, batch_size, num_workers):
+def _dataloader(data, sampler, batch_size, num_workers, use_cpu):
+    if use_cpu:
+        pin_memory = False 
+        num_workers = 0 
+    else:
+        pin_memory = True 
+        
     return torch.utils.data.DataLoader(
             data, batch_size=batch_size, num_workers=num_workers,
-            sampler=sampler, drop_last=True, pin_memory=True)
+            sampler=sampler, drop_last=True, pin_memory=pin_memory)
 
 
-def infinite_dataloader(data, sampler, batch_size, num_workers):
-    return _repeater(_dataloader(data, sampler, batch_size, num_workers))
+def infinite_dataloader(data, sampler, batch_size, num_workers, use_cpu):
+    return _repeater(_dataloader(data, sampler, batch_size, num_workers, use_cpu))
 
 
 class Wrap(object):
-    def __init__(self, data, sampler, batch_size, samples, num_workers):
-        self.data = infinite_dataloader(data, sampler, batch_size, num_workers)
+    def __init__(self, data, sampler, batch_size, samples, num_workers, use_cpu):
+        self.data = infinite_dataloader(data, sampler, batch_size, num_workers, use_cpu)
         self.samples = samples
 
     def __iter__(self):
diff --git a/carla_project/src/traffic_img_model.py b/carla_project/src/traffic_img_model.py
index b054c98..a656e90 100644
--- a/carla_project/src/traffic_img_model.py
+++ b/carla_project/src/traffic_img_model.py
@@ -94,12 +94,13 @@ def viz(batch, out, out_ctrl, target_cam, point_loss, ctrl_loss):
     return result
 
 
-class ImageModel(pl.LightningModule):
+class TrafficImageModel(pl.LightningModule):
     def __init__(self, hparams):
         super().__init__()
 
         self.hparams = hparams
         self.to_heatmap = ToHeatmap(hparams.heatmap_radius)
+        self.use_cpu = hparams.cpu
 
         # if teacher_path:
         #     self.teacher = MapModel.load_from_checkpoint(teacher_path)
@@ -265,10 +266,10 @@ class ImageModel(pl.LightningModule):
         return [optim], [scheduler]
 
     def train_dataloader(self):
-        return get_dataset(self.hparams.dataset_dir, True, self.hparams.batch_size, sample_by=self.hparams.sample_by)
+        return get_dataset(self.hparams.dataset_dir, True, self.hparams.batch_size, sample_by=self.hparams.sample_by, use_cpu=self.use_cpu)
 
     def val_dataloader(self):
-        return get_dataset(self.hparams.dataset_dir, False, self.hparams.batch_size, sample_by=self.hparams.sample_by)
+        return get_dataset(self.hparams.dataset_dir, False, self.hparams.batch_size, sample_by=self.hparams.sample_by, use_cpu=self.use_cpu)
 
     def state_dict(self):
         return {k: v for k, v in super().state_dict().items() if 'teacher' not in k}
@@ -285,12 +286,15 @@ def main(hparams):
     except:
         resume_from_checkpoint = None
 
-    model = ImageModel(hparams)
-    logger = WandbLogger(id=hparams.id, save_dir=str(hparams.save_dir), project='stage_2')
+    model = TrafficImageModel(hparams)
+    logger = WandbLogger(id=hparams.id, save_dir=str(hparams.save_dir), project='traffic_model')
     checkpoint_callback = ModelCheckpoint(hparams.save_dir, save_top_k=1)
 
     trainer = pl.Trainer(
-            gpus=-1, max_epochs=hparams.max_epochs,
+            # gpus=-1, 
+            gpus=None, 
+            accelerator="cpu",
+            max_epochs=hparams.max_epochs,
             resume_from_checkpoint=resume_from_checkpoint,
             logger=logger, checkpoint_callback=checkpoint_callback)
 
@@ -304,6 +308,7 @@ if __name__ == '__main__':
     parser.add_argument('--max_epochs', type=int, default=50)
     parser.add_argument('--save_dir', type=pathlib.Path, default='checkpoints')
     parser.add_argument('--id', type=str, default=uuid.uuid4().hex)
+    parser.add_argument('--cpu', action='store_true', default=False)
 
     # parser.add_argument('--teacher_path', type=pathlib.Path, required=True)
 
